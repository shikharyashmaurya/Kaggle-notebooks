{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Qwen/Qwen2.5-Coder-7B-Instruct","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:05:52.879740Z","iopub.execute_input":"2025-03-07T18:05:52.879939Z","iopub.status.idle":"2025-03-07T18:05:52.884087Z","shell.execute_reply.started":"2025-03-07T18:05:52.879919Z","shell.execute_reply":"2025-03-07T18:05:52.882930Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\", trust_remote_code=True)\n\n# Load model with memory optimization using bfloat16 precision\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-Coder-7B-Instruct\", \n    trust_remote_code=True, \n    torch_dtype=torch.bfloat16\n).cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:06:06.752045Z","iopub.execute_input":"2025-03-07T18:06:06.752326Z","iopub.status.idle":"2025-03-07T18:08:17.522038Z","shell.execute_reply.started":"2025-03-07T18:06:06.752303Z","shell.execute_reply":"2025-03-07T18:08:17.521290Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8bace82bec54736b41863f9ae54b852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d009f1e4c9748d1a9b8c2dc4204f3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687658fb31814d70a3d2e29d0f890e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728acf7e3a7d46de913719b745d240f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113e4a30594241749a7993adbd877450"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"025735036a904b208ec1216d1d775b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc83775feca24180908e76eab544719f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a3888858514992a44375048d39011e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d607ad5d2a4307b5f72834faedf5aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85de35f63b9f4b20b78316786eeda91a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"231687d1dad94255ae4f6edf7caf17a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f708a3158beb4d76920afbfc9d1e0ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a60f4798bd7445a4a4ae4910a3346a77"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Assume tokenizer and model are already loaded\n# tokenizer = AutoTokenizer.from_pretrained(\"some-chat-model\")\n# model = AutoModelForCausalLM.from_pretrained(\"some-chat-model\")\n\nmessages = [{'role': 'user', 'content': \"Write a Python function of bubble sort\"}]\n\n# Apply chat template and move to device\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n\n# Handle different possible return types\nif isinstance(inputs, torch.Tensor):\n    # If inputs is a tensor, assume it's input_ids\n    current_input = inputs\n    # Create attention_mask (all ones) if not provided\n    attention_mask = torch.ones_like(current_input, device=current_input.device)\nelif isinstance(inputs, dict):\n    # If inputs is a dictionary, access input_ids and attention_mask\n    current_input = inputs['input_ids']\n    attention_mask = inputs.get('attention_mask', torch.ones_like(current_input, device=current_input.device))\nelse:\n    raise ValueError(f\"Unexpected return type from apply_chat_template: {type(inputs)}\")\n\n# Ensure tensors are 2D (batch_size, sequence_length)\nif current_input.dim() == 1:\n    current_input = current_input.unsqueeze(0)\n    attention_mask = attention_mask.unsqueeze(0)\nelif current_input.dim() != 2:\n    raise ValueError(f\"input_ids must be 1D or 2D, got {current_input.dim()}D\")\n\n# Proceed with generation\ngenerated_tokens = []\n\nprint(\"Generating tokens:\")\nwhile len(generated_tokens) < 2048:\n    outputs = model.generate(\n        input_ids=current_input,\n        attention_mask=attention_mask,\n        max_new_tokens=1,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.95,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    next_token = outputs[0, -1]  # Last token in the sequence (batch_size=1)\n    generated_tokens.append(next_token)\n\n    if next_token == tokenizer.eos_token_id:\n        break\n\n    # Append the new token to the input sequence\n    next_token_tensor = next_token.unsqueeze(0).unsqueeze(0)  # Shape (1, 1)\n    current_input = torch.cat([current_input, next_token_tensor], dim=1)\n    attention_mask = torch.cat([attention_mask, torch.ones((1, 1), dtype=attention_mask.dtype, device=attention_mask.device)], dim=1)\n\n    # Print the decoded token\n    print(tokenizer.decode(next_token, skip_special_tokens=True), end=' ')\n\n# Decode and print the final generated code\ngenerated_code = tokenizer.decode(generated_tokens, skip_special_tokens=True)\nprint(\"\\n\\nFinal generated code:\\n\")\nprint(generated_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:08:25.152668Z","iopub.execute_input":"2025-03-07T18:08:25.153491Z","iopub.status.idle":"2025-03-07T18:10:45.445369Z","shell.execute_reply.started":"2025-03-07T18:08:25.153435Z","shell.execute_reply":"2025-03-07T18:10:45.444641Z"}},"outputs":[{"name":"stdout","text":"Generating tokens:\n def  bubble _sort (arr ):\n      n  =  len (arr )\n      #  Traverse  through  all  array  elements \n      for  i  in  range (n ):\n          #  Last  i  elements  are  already  in  place \n          for  j  in  range ( 0 ,  n -i - 1 ):\n              #  Traverse  the  array  from   0  to  n -i - 1 \n              #  Swap  if  the  element  found  is  greater  than  the  next  element \n              if  arr [j ]  >  arr [j + 1 ]:\n                  arr [j ],  arr [j + 1 ]  =  arr [j + 1 ],  arr [j ]\n      return  arr \n\n #  Example  usage :\n arr  =  [ 6 4 ,   3 4 ,   2 5 ,   1 2 ,   2 2 ,   1 1 ,   9 0 ]\n sorted _arr  =  bubble _sort (arr )\n print (\" Sorted  array  is :\",  sorted _arr ) \n\nFinal generated code:\n\ndef bubble_sort(arr):\n    n = len(arr)\n    # Traverse through all array elements\n    for i in range(n):\n        # Last i elements are already in place\n        for j in range(0, n-i-1):\n            # Traverse the array from 0 to n-i-1\n            # Swap if the element found is greater than the next element\n            if arr[j] > arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n    return arr\n\n# Example usage:\narr = [64, 34, 25, 12, 22, 11, 90]\nsorted_arr = bubble_sort(arr)\nprint(\"Sorted array is:\", sorted_arr)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}